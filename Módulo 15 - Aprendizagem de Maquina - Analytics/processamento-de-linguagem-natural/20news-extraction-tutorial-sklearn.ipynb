{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Working With Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##A fim de obter tempos de execução mais rápidos para este primeiro exemplo, trabalharemos em um conjunto de dados parcial com apenas 4 categorias das 20 disponíveis no conjunto de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Agora podemos carregar a lista de arquivos que correspondem a essas categorias da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "     categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twenty_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##O conjunto de dados retornado é um \"grupo\" (bunch) de scikit-learn: um objeto de suporte simples com campos que podem ser acessados ​​como chaves python dict ou atributos de objeto por conveniência, por exemplo, o target_names contém a lista dos nomes de categoria solicitados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Os próprios arquivos são carregados na memória no atributo \"data\". Para referência, os nomes dos arquivos também estão disponíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)\n",
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vamos imprimir as primeiras linhas do primeiro arquivo carregado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Algoritmos de aprendizado supervisionado exigirão um rótulo de categoria para cada documento no conjunto de treinamento. Nesse caso, a categoria é o nome do grupo de notícias que também é o nome da pasta que contém os documentos individuais. Por motivos de velocidade e eficiência de espaço, o scikit-learn carrega o atributo target como uma matriz de inteiros que corresponde ao índice do nome da categoria na lista target_names. O ID de categoria inteira de cada amostra é armazenado no atributo de destino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##É possível recuperar os nomes das categorias da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "     print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Você deve ter notado que as amostras foram embaralhadas aleatoriamente quando chamamos fetch_20newsgroups (..., shuffle = True, random_state = 42): isso é útil se você deseja selecionar apenas um subconjunto de amostras para treinar rapidamente um modelo e obter um primeiro ideia dos resultados antes de treinar novamente no conjunto de dados completo mais tarde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Extracting features from text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Para realizar o aprendizado de máquina em documentos de texto, primeiro precisamos transformar o conteúdo do texto em vetores de recursos numéricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sacos de palavras\n",
    "\n",
    "A maneira mais intuitiva de fazer isso é usar uma representação de sacos de palavras: Atribua um ID de número inteiro fixo para cada palavra que ocorre em qualquer documento do conjunto de treinamento (por exemplo, construindo um dicionário de palavras para índices inteiros). Para cada documento #i, conte o número de ocorrências de cada palavra w e armazene-o em X [i, j] como o valor da característica #j onde j é o índice da palavra w no dicionário. A representação dos pacotes de palavras implica que n_features é o número de palavras distintas no corpus: esse número é normalmente maior do que 100.000. Se n_samples == 10000, o armazenamento de X como uma matriz NumPy do tipo float32 exigiria 10000 x 100000 x 4 bytes = 4 GB em RAM, o que dificilmente é gerenciável nos computadores de hoje. Felizmente, a maioria dos valores em X serão zeros, pois, para um determinado documento, serão usados ​​menos de alguns milhares de palavras distintas. Por esse motivo, dizemos que pacotes de palavras são tipicamente conjuntos de dados esparsos de alta dimensão. Podemos economizar muita memória armazenando apenas as partes diferentes de zero dos vetores de recursos na memória. As matrizes scipy.sparse são estruturas de dados que fazem exatamente isso, e o scikit-learn tem suporte integrado para essas estruturas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Tokenização de texto\n",
    "\n",
    "Com scikit-learn Pré-processamento de texto, tokenização e filtragem de palavras irrelevantes estão todos incluídos no CountVectorizer, que cria um dicionário de recursos e transforma documentos em vetores de recursos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CountVectorizer suporta contagens de N-gramas de palavras ou caracteres consecutivos. Uma vez ajustado, o vetorizador construiu um dicionário de índices de recursos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##O valor do índice de uma palavra no vocabulário está relacionado à sua frequência em todo o corpus de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##De ocorrências a frequências\n",
    "\n",
    "A contagem de ocorrências é um bom começo, mas há um problema: documentos mais longos terão valores médios de contagem mais altos do que documentos mais curtos, embora possam falar sobre os mesmos tópicos. Para evitar essas discrepâncias em potencial, é suficiente dividir o número de ocorrências de cada palavra em um documento pelo número total de palavras no documento: esses novos recursos são chamados de tf para Frequências de termo. Outro refinamento além do tf é reduzir os pesos das palavras que ocorrem em muitos documentos no corpus e, portanto, são menos informativas do que aquelas que ocorrem apenas em uma parte menor do corpus. Essa redução é chamada de tf – idf para “Frequência do termo vezes frequência inversa do documento”. Tanto tf quanto tf – idf podem ser calculados da seguinte maneira usando TfidfTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##No código de exemplo acima, primeiro usamos o método fit (..) para ajustar nosso estimador aos dados e, em segundo lugar, o método transform (..) para transformar nossa matriz de contagem em uma representação tf-idf. Essas duas etapas podem ser combinadas para atingir o mesmo resultado final mais rápido, ignorando o processamento redundante. Isso é feito usando o método fit_transform (..) conforme mostrado abaixo e conforme mencionado na nota da seção anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Treinando um classificador \n",
    "\n",
    "Agora que temos nossos recursos, podemos treinar um classificador para tentar prever a categoria de uma postagem. Vamos começar com um classificador Bayes ingênuo, que fornece uma boa base para esta tarefa. O scikit-learn inclui várias variantes desse classificador; o mais adequado para contagem de palavras é a variante multinomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Para tentar prever o resultado em um novo documento, precisamos extrair os recursos usando quase a mesma cadeia de extração de recursos de antes. A diferença é que chamamos transform em vez de fit_transform nos transformadores, uma vez que eles já foram ajustados ao conjunto de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Building a pipeline\n",
    "\n",
    "In order to make the vectorizer => transformer => classifier easier to work with, scikit-learn provides a Pipeline class that behaves like a compound classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Os nomes vect, tfidf e clf (classificador) são arbitrários. Iremos usá-los para realizar o grid search para hiperparâmetros adequados abaixo. Agora podemos treinar o modelo com um único comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Avaliação do desempenho no conjunto de teste\n",
    "\n",
    "Avaliar a precisão preditiva do modelo é igualmente fácil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Atingimos 83,5% de precisão. Vamos ver se podemos fazer melhor com uma máquina de vetores de suporte linear (SVM), que é amplamente considerada como um dos melhores algoritmos de classificação de texto (embora também seja um pouco mais lenta do que o ingênuo Bayes). Podemos mudar o aluno simplesmente conectando um objeto classificador diferente em nosso pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
