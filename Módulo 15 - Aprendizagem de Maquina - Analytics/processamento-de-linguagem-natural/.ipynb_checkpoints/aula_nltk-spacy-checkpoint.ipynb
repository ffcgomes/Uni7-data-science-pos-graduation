{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\usuario\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: click in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'This is a large text. It is usually separated into sentences. We know that, but computers do not.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a large text.', 'It is usually separated into sentences.', 'We know that, but computers do not.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(string)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Olá!', 'Essa é uma frase em português.']\n"
     ]
    }
   ],
   "source": [
    "string_por = 'Olá! Essa é uma frase em português.'\n",
    "sentences = sent_tokenize(string_por, language='portuguese')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = webtext.raw('overheard.txt')\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = PunktSentenceTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "sents1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents2 = sent_tokenize(text)\n",
    "sents2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girl: But you already have a Big Mac...\n"
     ]
    }
   ],
   "source": [
    "print(sents1[678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girl: But you already have a Big Mac...\n",
      "Hobo: Oh, this is all theatrical.\n"
     ]
    }
   ],
   "source": [
    "print(sents2[678])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Hi! This is a simple sentence, and it can't be simpler than simpler!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'This', 'is', 'a', 'simple', 'sentence', ',', 'and', 'it', 'ca', \"n't\", 'be', 'simpler', 'than', 'simpler', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi!', 'This', 'is', 'a', 'simple', 'sentence,', 'and', 'it', \"can't\", 'be', 'simpler', 'than', 'simpler!!']\n"
     ]
    }
   ],
   "source": [
    "print(string.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'This', 'is', 'a', 'simple', 'sentence', ',', 'and', 'it', 'can', \"'\", 't', 'be', 'simpler', 'than', 'simpler', '!!']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Hi! This is a simple sentence, and it can't be simpler!! like 2 by 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'This', 'is', 'a', 'simple', 'sentence', 'and', 'it', \"can't\", 'be', 'simpler', 'like', '2', 'by', '2']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(string, \"[\\w']+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'This', 'is', 'a', 'simple', 'sentence', 'and', 'it', 'can', 't', 'be', 'simpler', 'like', '2', 'by', '2']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(string, \"\\w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '2']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(string, \"\\d+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'This', 'is', 'a', 'simple', 'sentence', ',', 'and', 'it', 'can', \"'\", 't', 'be', 'simpler', '!!', 'like', '2', 'by', '2']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'this', 'is', 'a', 'simple', 'sentence', 'and', 'it', 'ca', \"n't\", 'be', 'simpler', 'like', '2', 'by', '2']\n"
     ]
    }
   ],
   "source": [
    "print([w.lower() for w in word_tokenize(string) if w not in punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shop'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('shopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n",
      "eat\n",
      "ate\n"
     ]
    }
   ],
   "source": [
    "print(pst.stem('eated'))\n",
    "print(pst.stem('eating'))\n",
    "print(pst.stem('eat'))\n",
    "print(pst.stem('ate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n",
      "eat\n",
      "at\n"
     ]
    }
   ],
   "source": [
    "print(lst.stem('eated'))\n",
    "print(lst.stem('eating'))\n",
    "print(lst.stem('eat'))\n",
    "print(lst.stem('ate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "pst.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr\n",
      "corr\n",
      "corr\n"
     ]
    }
   ],
   "source": [
    "print(snow.stem('corrida'))\n",
    "print(snow.stem('correndo'))\n",
    "print(snow.stem('correr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corrid'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst.stem('corrida')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USUARIO/nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USUARIO/nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-3a81e7bc6a09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'families'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USUARIO/nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "lem.lemmatize('families')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USUARIO/nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USUARIO/nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-eaa25c341e2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0:20}{1:20}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Word\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Lemma\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"{0:20}{1:20}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwordnet_lemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USUARIO/nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USUARIO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun. Some mice are good\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\usuario\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (8.0.1)\n",
      "Requirement already satisfied: pathy in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (2.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (4.50.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pathy->spacy) (3.0.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pt-core-news-md==3.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_md-3.0.0/pt_core_news_md-3.0.0-py3-none-any.whl#egg=pt_core_news_md==3.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pt-core-news-md==3.0.0) (3.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (20.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (8.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: pathy in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pathy->spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->pt-core-news-md==3.0.0) (7.1.2)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você\testá\tbem\t?\t\n",
      "PRON\tAUX\tADV\tPUNCT\t\n",
      "Você\testar\tbem\t?\t\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "pos = ''\n",
    "lemma = ''\n",
    "for token in nlp(\"Você está bem?\"):\n",
    "    text += token.text + '\\t'\n",
    "    pos += token.pos_ + '\\t'\n",
    "    lemma += token.lemma_ + '\\t'\n",
    "print(text)\n",
    "print(pos)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'simple', 'sentence', 'ca', \"n't\", 'simpler', 'like', '2', '2']\n"
     ]
    }
   ],
   "source": [
    "print([w.lower() for w in word_tokenize(string) if (w not in punctuation) and (w.lower() not in stopwords.words('english'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'VBD'), ('eating', 'VBG'), ('dinners', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "string = 'I was eating dinners'\n",
    "tokens = word_tokenize(string)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Mark is studying at Stanford University in California and he started in September 2019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  is/VBZ\n",
      "  studying/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (GPE California/NNP)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  started/VBD\n",
      "  in/IN\n",
      "  September/NNP\n",
      "  2019/CD)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(nltk.pos_tag(word_tokenize(sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Bill Gates é um milionário.',\n",
    "             'Ele também é o fundador da Microsoft',\n",
    "             'O estado do Ceará deve firmar parceria com a Amazon no ano de 2021.',\n",
    "             'O Ceará Sporting Club é o melhor time do mundo.',\n",
    "             'Em 09/10/2021 será um dia de primavera',\n",
    "             'Bill Gates não terminou a faculdade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "for sent in sentences:\n",
    "    doc = nlp(sent)\n",
    "    for i in doc.ents:\n",
    "        entities.append((str(i.ents[0]).lstrip().rstrip(), i.ents[0].label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bill Gates', 'PER'),\n",
       " ('Microsoft', 'ORG'),\n",
       " ('estado do Ceará', 'LOC'),\n",
       " ('Amazon', 'ORG'),\n",
       " ('Ceará Sporting Club', 'ORG'),\n",
       " ('Bill Gates', 'PER')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bill Gates': 2}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict()\n",
    "for ent in entities:\n",
    "    if(ent[1] == 'PER'):\n",
    "        if(ent[0] not in d.keys()):\n",
    "            d[ent[0]] = 1\n",
    "        else:\n",
    "            d[ent[0]] += 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['Bill Gates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweets.txt',encoding=\"utf8\") as f:\n",
    "    sentences = f.readlines()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "for sent in sentences:\n",
    "    doc = nlp(sent)\n",
    "    for i in doc.ents:\n",
    "        entities.append((str(i.ents[0]).lstrip().rstrip(), i.ents[0].label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Acompanhe': 3,\n",
       " 'Camilo Andrés Vergara L': 102,\n",
       " 'Acciones': 96,\n",
       " 'Wanda': 116,\n",
       " 'Amanhã': 1,\n",
       " 'Messi': 2,\n",
       " 'Neymar': 1,\n",
       " 'Cleber': 13,\n",
       " 'Rogério Ceni': 24,\n",
       " 'Marlon': 1,\n",
       " 'Deus': 5,\n",
       " 'Vina': 76,\n",
       " 'Boas': 3,\n",
       " 'Idilvan Fortaleza': 1,\n",
       " 'IMPORTANTÍSSIMA': 3,\n",
       " 'Fernando Prass': 8,\n",
       " 'Pablo Iglesias': 23,\n",
       " 'Palpites': 1,\n",
       " \"D'això\": 3,\n",
       " 'Vaupés': 1,\n",
       " 'Villavice': 1,\n",
       " 'Tinga': 2,\n",
       " 'Aca': 18,\n",
       " 'Yuri César': 10,\n",
       " 'Prass': 2,\n",
       " 'Samuel Xavier': 5,\n",
       " 'Tiago Pagnussat': 1,\n",
       " 'Luiz Otávio': 2,\n",
       " 'Bruno Pacheco': 1,\n",
       " 'Charles': 74,\n",
       " 'Felipe Alves': 20,\n",
       " 'Gabriel Dias': 7,\n",
       " 'Quintero': 2,\n",
       " 'Paulão': 13,\n",
       " 'Bruno Melo': 4,\n",
       " 'Juninho': 5,\n",
       " 'Ojalá': 1,\n",
       " 'Isaías': 1,\n",
       " 'Isabel 32x30': 1,\n",
       " 'Zikei': 1,\n",
       " 'Cállese': 17,\n",
       " 'Fortaleza EC': 1,\n",
       " '@elClubOlimpia Totalmente': 1,\n",
       " 'Ortíz': 1,\n",
       " 'PUTA': 1,\n",
       " 'Cléber': 10,\n",
       " 'José Carlos da Penha': 1,\n",
       " 'Herodes': 2,\n",
       " 'Juan el Bautista': 2,\n",
       " 'Wanda Seux': 1,\n",
       " 'WP9': 1,\n",
       " 'Everaldo': 1,\n",
       " 'Rev. Julio L. https://t.co/wZA4W2puaX': 1,\n",
       " 'Facundo': 4,\n",
       " 'MetroDeCaracas MG': 5,\n",
       " 'Es': 7,\n",
       " 'Quiero': 11,\n",
       " 'Juan Rodríguez Suárez': 5,\n",
       " 'Jajá': 1,\n",
       " 'Acompanhe Ceará': 1,\n",
       " 'Fortaleza': 5,\n",
       " 'Alfonso Cepeda Salas 🔴': 3,\n",
       " 'Fortaleza(há': 1,\n",
       " '@melonisyellow': 1,\n",
       " 'Odo do Rogério': 1,\n",
       " 'David': 41,\n",
       " 'Arnaldo': 2,\n",
       " 'Aquele': 3,\n",
       " 'PORRA': 1,\n",
       " 'BORA': 1,\n",
       " 'Diálogo Interamericano': 30,\n",
       " 'Professor Guto': 1,\n",
       " 'Professor Rogério': 1,\n",
       " 'Isabel Curiel Turbay': 2,\n",
       " 'Los': 3,\n",
       " 'Paulo Luz': 1,\n",
       " 'Menino': 1,\n",
       " 'Alguém': 3,\n",
       " '@hildeadolfo': 1,\n",
       " 'Su': 9,\n",
       " 'Felicitaciones': 1,\n",
       " 'Bola': 20,\n",
       " 'Rubalcaba tuvo mucho': 1,\n",
       " 'Ceni': 13,\n",
       " 'Etruria': 1,\n",
       " 'Link AceStream': 2,\n",
       " 'Líder Vietnamita': 5,\n",
       " 'Silêncio': 1,\n",
       " 'Senhor Guto Ferreira': 1,\n",
       " 'Hahaha': 1,\n",
       " 'Ceará vs Fortaleza': 1,\n",
       " 'ARENA CASTELÃO': 4,\n",
       " 'Fernando Sobral': 12,\n",
       " 'Cartola': 11,\n",
       " 'María Servini': 2,\n",
       " 'Fortaleza 🖥👀': 1,\n",
       " 'Espíritu': 1,\n",
       " 'QEPD': 1,\n",
       " 'Escanteio': 4,\n",
       " 'Leão': 2,\n",
       " '@abc_es Magnífica': 2,\n",
       " 'Disso': 1,\n",
       " 'Alguna': 2,\n",
       " '👴🏼': 2,\n",
       " '🔃 Vina': 2,\n",
       " '🍻': 1,\n",
       " '🔥👀': 1,\n",
       " 'W. Paulista': 2,\n",
       " 'Homenaje': 6,\n",
       " 'Mandou': 1,\n",
       " 'Garzón': 2,\n",
       " 'https://t.co/GK6zU06gDl': 1,\n",
       " 'Felipe Conceição': 1,\n",
       " 'Cleber do Ceará': 8,\n",
       " 'CATANDUVA': 2,\n",
       " 'Ceará JOYSTICK': 1,\n",
       " 'Pide Fortaleza': 1,\n",
       " 'Más': 1,\n",
       " 'Luis Maya Doro': 1,\n",
       " 'Hay': 1,\n",
       " 'Marielle Franco': 51,\n",
       " '@psol50': 5,\n",
       " 'Ali': 1,\n",
       " '☝🏽✨': 1,\n",
       " \"'Le\": 1,\n",
       " 'Lula': 1,\n",
       " 'Cadê': 1,\n",
       " 'Yuri Cesar': 4,\n",
       " 'Romarinho': 7,\n",
       " '@chechechacon Es posible': 1,\n",
       " 'Arrasca': 1,\n",
       " 'Luís Fabiano': 1,\n",
       " 'Diogo': 1,\n",
       " 'Ximena': 1,\n",
       " 'Dirigentes': 1,\n",
       " '📺⚽': 1,\n",
       " 'Clás': 1,\n",
       " 'Yuri césar do Fortaleza': 1,\n",
       " 'Erival': 1,\n",
       " 'Zanny': 1,\n",
       " 'Otero': 1,\n",
       " 'Vivan': 1,\n",
       " 'Ángel Guardía': 1,\n",
       " 'Tiago Nunes': 1,\n",
       " '@alepermon Estoy': 1,\n",
       " 'Melissa Mark-Viverito': 1,\n",
       " 'Ricardo Rossell': 1,\n",
       " '@cactusabrazable Sii': 1,\n",
       " 'KKKKKKKKKKJJJJ': 1,\n",
       " 'Gringo': 1,\n",
       " 'Papá de Solange': 1,\n",
       " 'COVID19': 1,\n",
       " 'Sin': 4,\n",
       " 'Felipe': 6,\n",
       " 'Mito Ceni': 1,\n",
       " 'Dr. Mata': 1,\n",
       " 'Y Evo': 76,\n",
       " 'Oswaldo': 1,\n",
       " 'Rei': 2,\n",
       " 'Sirius Black Banco Central': 1,\n",
       " 'Ceará': 5,\n",
       " 'R. Ceni': 1,\n",
       " 'Obtienen la': 1,\n",
       " 'Covid-19': 6,\n",
       " 'Guto Ferreira': 10,\n",
       " 'Marcinho': 1,\n",
       " 'Fabrício Bruno': 1,\n",
       " '@carmenchu33': 1,\n",
       " 'Lu': 1,\n",
       " 'Rei Vai': 1,\n",
       " 'Renato': 1,\n",
       " 'Si': 2,\n",
       " 'Jallalla David': 1,\n",
       " 'Lucas Ribei': 1,\n",
       " '🥵': 1,\n",
       " 'Santa Glori': 1,\n",
       " 'Fabio Pinzón': 1,\n",
       " 'Jairo Andr': 1,\n",
       " 'Karen': 1,\n",
       " 'Venhaaaa': 1,\n",
       " 'Jogar': 3,\n",
       " 'Cabral': 1,\n",
       " 'Un': 4,\n",
       " 'María de Villasmil Paz y Luz': 1,\n",
       " 'Van': 1,\n",
       " 'Adriana': 2,\n",
       " 'Esa': 2,\n",
       " 'Pardo Bazán': 1,\n",
       " 'los Franco': 1,\n",
       " '🙌🏼': 2,\n",
       " 'Bárbara Drumond': 3,\n",
       " 'Bob Jeff': 2,\n",
       " 'Bueno': 1,\n",
       " 'Ayer': 2,\n",
       " 'Nicolás!!!Mucha': 1,\n",
       " 'Te': 1,\n",
       " 'Martha y punto': 1,\n",
       " 'Debes': 1,\n",
       " 'Leandro Carvalho': 2,\n",
       " 'Peter Crouch': 1,\n",
       " 'Losiento mucho': 1,\n",
       " 'Deivid': 4,\n",
       " 'Aylan Kurdi': 4,\n",
       " 'Capitão': 1,\n",
       " 'https://t.co/rQzpF65NXv': 1,\n",
       " '🎷🥁🎶': 8,\n",
       " 'William Oliveira': 2,\n",
       " 'Marina y Fabio': 1,\n",
       " 'Eva': 1,\n",
       " 'Agustín Rojas': 1,\n",
       " 'quien': 1,\n",
       " 'Ciro Gomes': 3,\n",
       " 'iPho': 6,\n",
       " 'Romarinho do Fortaleza': 2,\n",
       " '🤡🤡🤡🤡🤡🤡': 5,\n",
       " 'Kleber': 1,\n",
       " 'y Marcos Antil': 1,\n",
       " 'Resignación y Fortaleza': 1,\n",
       " '@babymatosa Maravilha': 1,\n",
       " 'Sampaio Corrêa': 1,\n",
       " 'Atleta': 1,\n",
       " 'Carlos Eduardo': 1,\n",
       " 'Atleta Deivid do Fortaleza': 1,\n",
       " 'Lila': 1,\n",
       " 'Adriana Bernal': 2,\n",
       " 'Victor Bernal': 2,\n",
       " '🤡🤡🤡': 2,\n",
       " 'Cantinho Acadêmico': 1,\n",
       " 'Tocavita': 12,\n",
       " 'Fortaleza X Ceará': 1,\n",
       " 'Vamos': 6,\n",
       " 'Lutare': 6,\n",
       " 'José Félix Ribas': 1,\n",
       " 'Birgada Henry Reeve': 1,\n",
       " \"@relibertad 'Templanza\": 1,\n",
       " 'Detalhe': 1,\n",
       " 'Barçaleza': 1,\n",
       " 'Mis': 1,\n",
       " '💖': 2,\n",
       " '🥅 Vinícius': 3,\n",
       " 'Nossa': 1,\n",
       " 'Apostei': 2,\n",
       " 'Rogério': 12,\n",
       " '@kayro63': 1,\n",
       " '👍👍👍👍': 1,\n",
       " 'Clebão': 1,\n",
       " 'Vinícius ⚽': 1,\n",
       " 'Rogerio Ceni': 1,\n",
       " 'Vinícius do Ceará': 7,\n",
       " 'Vinicius': 18,\n",
       " 'Vin': 1,\n",
       " 'Conto': 1,\n",
       " 'Professor Gut': 1,\n",
       " 'Vinicius do Ceará': 6,\n",
       " 'Samba': 1,\n",
       " 'Vinícius': 34,\n",
       " 'Medo': 2,\n",
       " 'Vinicius Vina': 2,\n",
       " 'Mário Thiago': 1,\n",
       " 'Abrazo': 1,\n",
       " 'Brun': 1,\n",
       " '😔': 1,\n",
       " 'Luiz Flávio de Oliveira': 2,\n",
       " 'Charles do Ceará': 1,\n",
       " 'Vinícius Goes': 1,\n",
       " 'Craque': 13,\n",
       " 'Fabio Sanches': 1,\n",
       " 'David Moyes': 1,\n",
       " 'Vinicius do Ceará 🤪': 1,\n",
       " '🅰': 1,\n",
       " '@fogareu00': 1,\n",
       " 'Jajá o': 1,\n",
       " 'Goleiro Felipe Alves': 2,\n",
       " 'Luiz Flávio': 1,\n",
       " 'NENHUMA': 1,\n",
       " 'Porra': 2,\n",
       " 'Mucha': 1,\n",
       " 'Viniccius': 1,\n",
       " 'Mano': 2,\n",
       " 'Acertei': 1,\n",
       " 'Les': 1,\n",
       " '🙅🏾\\u200d♀️🙅🏾\\u200d♀️🙅🏾\\u200d♀️': 1,\n",
       " 'Perder': 1,\n",
       " 'Dün': 1,\n",
       " 'Guto': 6,\n",
       " 'Kkk': 1,\n",
       " 'Davi': 1,\n",
       " 'Yuri': 3,\n",
       " 'Mi': 2,\n",
       " '🤡🤡🤡🤡🤡🤡🤡🤡': 1,\n",
       " 'Evanilson': 4,\n",
       " '🤡🤡🤡🤡': 1,\n",
       " 'Karamasov': 1,\n",
       " 'Dostoyevski': 1,\n",
       " 'QEPD😞': 1,\n",
       " 'Ceará S': 1,\n",
       " 'Ednardo': 1,\n",
       " 'Cea': 2,\n",
       " 'Sérgio Moro': 1,\n",
       " 'DELE': 10,\n",
       " 'Gabriel do Fortaleza': 1,\n",
       " 'Anem Fortaleza': 1,\n",
       " 'João o Fortaleza': 1,\n",
       " '@lucaascartaxo Pior': 1,\n",
       " 'Pra': 1,\n",
       " 'Che Che Súper Wanda': 1,\n",
       " '@alvaromartins87': 2,\n",
       " 'Mateus Gonçalves do Ceará': 1,\n",
       " 'Chupa': 1,\n",
       " 'Goleiro do Fortaleza': 2,\n",
       " 'Valhalla': 1,\n",
       " 'Vinícius Vina': 1,\n",
       " 'Louis Tomlinson': 1,\n",
       " 'INCOMPLETOS': 1,\n",
       " 'Maduro': 1,\n",
       " 'Iti': 1,\n",
       " 'Q VC': 1,\n",
       " 'Dra Delsa': 1,\n",
       " '@lucaascartaxo Acabei': 1,\n",
       " 'Solange Musse y su papá?': 1,\n",
       " 'Derrote': 1,\n",
       " 'BARBARIZOU': 1,\n",
       " '@_neigrah Botei': 1,\n",
       " 'Jorge Moyano Vega': 1,\n",
       " 'Juan': 1,\n",
       " 'Rojas': 1,\n",
       " 'Torcedora': 1,\n",
       " 'Bruyne': 1,\n",
       " 'Renan Oliveira': 1,\n",
       " 'Shaylon': 1,\n",
       " 'Guilherme': 1,\n",
       " 'Nao': 1,\n",
       " '😌': 1,\n",
       " 'Achei': 1,\n",
       " '👊🏽': 1,\n",
       " 'Gordiola': 4,\n",
       " 'Escalei': 4,\n",
       " 'Vinicius ⚽️ 44': 1,\n",
       " 'Cópia': 1,\n",
       " 'https://t.co/NlJBYaUSs7': 1,\n",
       " 'Gobernador Roselló de la Fortaleza': 1,\n",
       " 'Vicente Zeballos aunque': 9,\n",
       " 'Perdi': 3,\n",
       " 'ARENA CASTELÃO!': 2,\n",
       " '📸 Kely Pereira': 1,\n",
       " 'Luiz Otavio': 1,\n",
       " 'Cit': 1,\n",
       " 'Parec': 1,\n",
       " '🤡🤡🤡🤡🤡🤡🤡': 1,\n",
       " 'Campos Neto': 2,\n",
       " 'Breno': 1,\n",
       " \"Vinícius 'Vina\": 1,\n",
       " 'Ritinha': 1,\n",
       " 'Jehová': 2,\n",
       " 'Cléber do Ceará 🤡🤡🤡🤡': 1,\n",
       " '«Recuerde': 1,\n",
       " 'Deitado': 1,\n",
       " 'Reyes Católicos': 1,\n",
       " 'Isabel': 1,\n",
       " 'Samuel': 1,\n",
       " 'Solange': 1,\n",
       " 'Blas Correas': 1,\n",
       " 'Lances': 1,\n",
       " '@lukecastelao': 1,\n",
       " 'Mamãe Falei': 1,\n",
       " '@pawsksj': 1,\n",
       " 'Pottker': 1,\n",
       " 'Bruno Nazário': 1,\n",
       " 'Jesús': 1,\n",
       " 'Justo': 1,\n",
       " 'Ricardinho do Ceará': 1,\n",
       " 'Fernando Sobral do Ceará': 2,\n",
       " 'Prezada Renata': 1,\n",
       " 'Norm': 1,\n",
       " 'Schiaretti': 1,\n",
       " '🤡🤡': 2,\n",
       " 'Dale': 1,\n",
       " 'Timot': 1,\n",
       " 'Léo Pereira': 1,\n",
       " 'Ceará Quitero': 1,\n",
       " 'Jackson': 1,\n",
       " 'Marisa': 1,\n",
       " 'Morales Garcés': 2,\n",
       " 'Las': 1,\n",
       " 'Bella': 1,\n",
       " 'Loko': 1,\n",
       " 'Gareth Bale': 1,\n",
       " 'Tio Eucli': 2,\n",
       " 'Sr Noroña': 1,\n",
       " 'Gracias Sole': 6,\n",
       " 'Rezaré': 6,\n",
       " 'Sufrimos': 1,\n",
       " 'Indultos': 1,\n",
       " 'Vai': 1,\n",
       " 'Vagabundo': 1,\n",
       " 'Confiaré y': 1,\n",
       " 'Querías': 1,\n",
       " 'Vásquez': 2,\n",
       " '@ocomenteiro Poderia': 1,\n",
       " 'Japonés Kawasaki': 1,\n",
       " 'Câmera': 1,\n",
       " 'Roberto': 1,\n",
       " 'Papa Inocencio XI': 1,\n",
       " 'Cássio': 1,\n",
       " 'EstoNoSeraTendencia': 1,\n",
       " 'Sarrafiore': 7,\n",
       " 'Bate': 1,\n",
       " 'Ido dónde quiera': 1,\n",
       " 'KKKKKKKKKKKKKKKKKK': 1,\n",
       " 'Gina': 1,\n",
       " 'Poe': 1,\n",
       " 'Hermano': 1,\n",
       " 'Bicho': 1,\n",
       " 'Ferna': 1,\n",
       " 'Ciro': 2,\n",
       " 'Andréa Bardawil': 2,\n",
       " 'Oswald Barroso': 2,\n",
       " 'Nathan': 1,\n",
       " 'Aaaaaaaaah': 1,\n",
       " 'Jano Cleyson': 1,\n",
       " 'Senhor': 2,\n",
       " 'Hurtado': 3,\n",
       " 'Jan Hurtado': 3,\n",
       " 'Cachaceiro vc': 1,\n",
       " 'Daniel Zellhube': 1,\n",
       " 'Salmito Filho': 1,\n",
       " 'Rapaz': 4,\n",
       " '😔😔✋🏻': 1,\n",
       " 'Fagner': 2,\n",
       " 'Coro Gomes': 1,\n",
       " 'Rael Barbosa de Jacarequara Cedral': 1,\n",
       " 'Martínez': 1,\n",
       " 'Mailton': 1,\n",
       " 'Polariza': 1,\n",
       " 'También': 1,\n",
       " 'Pedro': 4,\n",
       " 'Vázquez': 1,\n",
       " 'Pohessa': 1,\n",
       " 'Melissa': 1,\n",
       " 'Véi': 1,\n",
       " 'Jesús y': 1,\n",
       " 'Rafael Sóbis': 1,\n",
       " 'Athleti': 2,\n",
       " 'Veronica Bailone Viceintendenta': 1,\n",
       " 'Laird Hamilton': 1,\n",
       " '@crvgeloi': 1,\n",
       " 'Ceará Fabinho': 1,\n",
       " 'W. Oliveira': 1,\n",
       " 'Sobis': 1,\n",
       " 'Melissa Dios': 1,\n",
       " 'Mariano': 1,\n",
       " 'Fragapane': 1,\n",
       " 'Sóbis': 1,\n",
       " 'Sebastián de Belalcázar': 1,\n",
       " 'Ricardo Rivera Ardila': 1,\n",
       " 'André Henning': 1,\n",
       " 'Kkkk': 1,\n",
       " 'Cléber 🤡🤡🤡🤡🤡🤡🤡': 1,\n",
       " 'Triste 😢 QEPD el': 1,\n",
       " 'Juan Carlos Villalba Barragán': 1,\n",
       " 'Gabriela Villalba y mucha fortaleza 🕊': 1,\n",
       " 'Mujeeer': 1,\n",
       " 'Derrubo': 1,\n",
       " 'Mariano Vásquez': 1,\n",
       " 'Fernando': 1,\n",
       " '@mundodeumleao Tou': 1,\n",
       " 'Phatético': 1,\n",
       " 'Boletim': 1,\n",
       " 'Puta': 2,\n",
       " 'Luan Peres': 1,\n",
       " 'Patric Brey': 1,\n",
       " 'Giovani': 1,\n",
       " 'Ricardo': 1,\n",
       " 'Gil': 2,\n",
       " 'Dios es': 1,\n",
       " 'F. Baxola': 1,\n",
       " 'Darío Cvitanich': 1,\n",
       " 'Querido Facundo': 1,\n",
       " 'Leí': 1,\n",
       " 'Mlr': 1,\n",
       " 'Mary': 1,\n",
       " 'Matheus': 1,\n",
       " 'Mis Oraciones': 1,\n",
       " \"Cristo Jesús 'Por lo demás\": 1,\n",
       " 'Sra. Chávez': 1,\n",
       " 'Uds es': 1,\n",
       " 'Cléber do Ceará': 1,\n",
       " 'Experimenté': 1,\n",
       " 'Lupin': 1,\n",
       " 'Jigen': 1,\n",
       " 'Cerri': 1,\n",
       " 'Vc': 1,\n",
       " 'Anderson': 1,\n",
       " 'Dale Sr.': 1,\n",
       " 'Paz y pido': 1,\n",
       " 'Alonso': 1,\n",
       " 'Jorge Thielen Armand': 2,\n",
       " 'Piada': 1,\n",
       " 'Pleno': 1,\n",
       " 'Falhei': 1,\n",
       " 'KKKKK': 1,\n",
       " 'David Corrêa': 1,\n",
       " 'Jesus': 1,\n",
       " 'Bia': 1,\n",
       " 'Falou': 1,\n",
       " 'Retroescavadeira': 1,\n",
       " 'Venham': 1,\n",
       " 'Fortaleza VAI TOMAR NO CU #CEAxFOR': 1,\n",
       " 'Juan Fdo Ocampo': 1,\n",
       " '@eunaosoueuta': 1}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict()\n",
    "for ent in entities:\n",
    "    if(ent[1] == 'PER'):\n",
    "        if(ent[0] not in d.keys()):\n",
    "            d[ent[0]] = 1\n",
    "        else:\n",
    "            d[ent[0]] += 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wanda 116\n",
      "Camilo Andrés Vergara L 102\n",
      "Acciones 96\n",
      "Vina 76\n",
      "Y Evo 76\n",
      "Charles 74\n",
      "Marielle Franco 51\n",
      "David 41\n",
      "Vinícius 34\n",
      "Diálogo Interamericano 30\n",
      "Rogério Ceni 24\n",
      "Pablo Iglesias 23\n",
      "Felipe Alves 20\n",
      "Bola 20\n",
      "Aca 18\n",
      "Vinicius 18\n",
      "Cállese 17\n",
      "Cleber 13\n",
      "Paulão 13\n",
      "Ceni 13\n",
      "Craque 13\n",
      "Fernando Sobral 12\n",
      "Tocavita 12\n",
      "Rogério 12\n",
      "Quiero 11\n",
      "Cartola 11\n",
      "Yuri César 10\n",
      "Cléber 10\n",
      "Guto Ferreira 10\n",
      "DELE 10\n",
      "Su 9\n",
      "Vicente Zeballos aunque 9\n",
      "Fernando Prass 8\n",
      "Cleber do Ceará 8\n",
      "🎷🥁🎶 8\n",
      "Gabriel Dias 7\n",
      "Es 7\n",
      "Romarinho 7\n",
      "Vinícius do Ceará 7\n",
      "Sarrafiore 7\n",
      "Homenaje 6\n",
      "Felipe 6\n",
      "Covid-19 6\n",
      "iPho 6\n",
      "Vamos 6\n",
      "Lutare 6\n",
      "Vinicius do Ceará 6\n",
      "Guto 6\n",
      "Gracias Sole 6\n",
      "Rezaré 6\n",
      "Deus 5\n",
      "Samuel Xavier 5\n",
      "Juninho 5\n",
      "MetroDeCaracas MG 5\n",
      "Juan Rodríguez Suárez 5\n",
      "Fortaleza 5\n",
      "Líder Vietnamita 5\n",
      "@psol50 5\n",
      "Ceará 5\n",
      "🤡🤡🤡🤡🤡🤡 5\n",
      "Bruno Melo 4\n",
      "Facundo 4\n",
      "ARENA CASTELÃO 4\n",
      "Escanteio 4\n",
      "Yuri Cesar 4\n",
      "Sin 4\n",
      "Un 4\n",
      "Deivid 4\n",
      "Aylan Kurdi 4\n",
      "Evanilson 4\n",
      "Gordiola 4\n",
      "Escalei 4\n",
      "Rapaz 4\n",
      "Pedro 4\n",
      "Acompanhe 3\n",
      "Boas 3\n",
      "IMPORTANTÍSSIMA 3\n",
      "D'això 3\n",
      "Alfonso Cepeda Salas 🔴 3\n",
      "Aquele 3\n",
      "Los 3\n",
      "Alguém 3\n",
      "Jogar 3\n",
      "Bárbara Drumond 3\n",
      "Ciro Gomes 3\n",
      "🥅 Vinícius 3\n",
      "Yuri 3\n",
      "Perdi 3\n",
      "Hurtado 3\n",
      "Jan Hurtado 3\n",
      "Messi 2\n",
      "Tinga 2\n",
      "Prass 2\n",
      "Luiz Otávio 2\n",
      "Quintero 2\n",
      "Herodes 2\n",
      "Juan el Bautista 2\n",
      "Arnaldo 2\n",
      "Isabel Curiel Turbay 2\n",
      "Link AceStream 2\n",
      "María Servini 2\n",
      "Leão 2\n",
      "@abc_es Magnífica 2\n",
      "Alguna 2\n",
      "👴🏼 2\n",
      "🔃 Vina 2\n",
      "W. Paulista 2\n",
      "Garzón 2\n",
      "CATANDUVA 2\n",
      "Rei 2\n",
      "Si 2\n",
      "Adriana 2\n",
      "Esa 2\n",
      "🙌🏼 2\n",
      "Bob Jeff 2\n",
      "Ayer 2\n",
      "Leandro Carvalho 2\n",
      "William Oliveira 2\n",
      "Romarinho do Fortaleza 2\n",
      "Adriana Bernal 2\n",
      "Victor Bernal 2\n",
      "🤡🤡🤡 2\n",
      "💖 2\n",
      "Apostei 2\n",
      "Medo 2\n",
      "Vinicius Vina 2\n",
      "Luiz Flávio de Oliveira 2\n",
      "Goleiro Felipe Alves 2\n",
      "Porra 2\n",
      "Mano 2\n",
      "Mi 2\n",
      "Cea 2\n",
      "@alvaromartins87 2\n",
      "Goleiro do Fortaleza 2\n",
      "ARENA CASTELÃO! 2\n",
      "Campos Neto 2\n",
      "Jehová 2\n",
      "Fernando Sobral do Ceará 2\n",
      "🤡🤡 2\n",
      "Morales Garcés 2\n",
      "Tio Eucli 2\n",
      "Vásquez 2\n",
      "Ciro 2\n",
      "Andréa Bardawil 2\n",
      "Oswald Barroso 2\n",
      "Senhor 2\n",
      "Fagner 2\n",
      "Athleti 2\n",
      "Puta 2\n",
      "Gil 2\n",
      "Jorge Thielen Armand 2\n",
      "Amanhã 1\n",
      "Neymar 1\n",
      "Marlon 1\n",
      "Idilvan Fortaleza 1\n",
      "Palpites 1\n",
      "Vaupés 1\n",
      "Villavice 1\n",
      "Tiago Pagnussat 1\n",
      "Bruno Pacheco 1\n",
      "Ojalá 1\n",
      "Isaías 1\n",
      "Isabel 32x30 1\n",
      "Zikei 1\n",
      "Fortaleza EC 1\n",
      "@elClubOlimpia Totalmente 1\n",
      "Ortíz 1\n",
      "PUTA 1\n",
      "José Carlos da Penha 1\n",
      "Wanda Seux 1\n",
      "WP9 1\n",
      "Everaldo 1\n",
      "Rev. Julio L. https://t.co/wZA4W2puaX 1\n",
      "Jajá 1\n",
      "Acompanhe Ceará 1\n",
      "Fortaleza(há 1\n",
      "@melonisyellow 1\n",
      "Odo do Rogério 1\n",
      "PORRA 1\n",
      "BORA 1\n",
      "Professor Guto 1\n",
      "Professor Rogério 1\n",
      "Paulo Luz 1\n",
      "Menino 1\n",
      "@hildeadolfo 1\n",
      "Felicitaciones 1\n",
      "Rubalcaba tuvo mucho 1\n",
      "Etruria 1\n",
      "Silêncio 1\n",
      "Senhor Guto Ferreira 1\n",
      "Hahaha 1\n",
      "Ceará vs Fortaleza 1\n",
      "Fortaleza 🖥👀 1\n",
      "Espíritu 1\n",
      "QEPD 1\n",
      "Disso 1\n",
      "🍻 1\n",
      "🔥👀 1\n",
      "Mandou 1\n",
      "https://t.co/GK6zU06gDl 1\n",
      "Felipe Conceição 1\n",
      "Ceará JOYSTICK 1\n",
      "Pide Fortaleza 1\n",
      "Más 1\n",
      "Luis Maya Doro 1\n",
      "Hay 1\n",
      "Ali 1\n",
      "☝🏽✨ 1\n",
      "'Le 1\n",
      "Lula 1\n",
      "Cadê 1\n",
      "@chechechacon Es posible 1\n",
      "Arrasca 1\n",
      "Luís Fabiano 1\n",
      "Diogo 1\n",
      "Ximena 1\n",
      "Dirigentes 1\n",
      "📺⚽ 1\n",
      "Clás 1\n",
      "Yuri césar do Fortaleza 1\n",
      "Erival 1\n",
      "Zanny 1\n",
      "Otero 1\n",
      "Vivan 1\n",
      "Ángel Guardía 1\n",
      "Tiago Nunes 1\n",
      "@alepermon Estoy 1\n",
      "Melissa Mark-Viverito 1\n",
      "Ricardo Rossell 1\n",
      "@cactusabrazable Sii 1\n",
      "KKKKKKKKKKJJJJ 1\n",
      "Gringo 1\n",
      "Papá de Solange 1\n",
      "COVID19 1\n",
      "Mito Ceni 1\n",
      "Dr. Mata 1\n",
      "Oswaldo 1\n",
      "Sirius Black Banco Central 1\n",
      "R. Ceni 1\n",
      "Obtienen la 1\n",
      "Marcinho 1\n",
      "Fabrício Bruno 1\n",
      "@carmenchu33 1\n",
      "Lu 1\n",
      "Rei Vai 1\n",
      "Renato 1\n",
      "Jallalla David 1\n",
      "Lucas Ribei 1\n",
      "🥵 1\n",
      "Santa Glori 1\n",
      "Fabio Pinzón 1\n",
      "Jairo Andr 1\n",
      "Karen 1\n",
      "Venhaaaa 1\n",
      "Cabral 1\n",
      "María de Villasmil Paz y Luz 1\n",
      "Van 1\n",
      "Pardo Bazán 1\n",
      "los Franco 1\n",
      "Bueno 1\n",
      "Nicolás!!!Mucha 1\n",
      "Te 1\n",
      "Martha y punto 1\n",
      "Debes 1\n",
      "Peter Crouch 1\n",
      "Losiento mucho 1\n",
      "Capitão 1\n",
      "https://t.co/rQzpF65NXv 1\n",
      "Marina y Fabio 1\n",
      "Eva 1\n",
      "Agustín Rojas 1\n",
      "quien 1\n",
      "Kleber 1\n",
      "y Marcos Antil 1\n",
      "Resignación y Fortaleza 1\n",
      "@babymatosa Maravilha 1\n",
      "Sampaio Corrêa 1\n",
      "Atleta 1\n",
      "Carlos Eduardo 1\n",
      "Atleta Deivid do Fortaleza 1\n",
      "Lila 1\n",
      "Cantinho Acadêmico 1\n",
      "Fortaleza X Ceará 1\n",
      "José Félix Ribas 1\n",
      "Birgada Henry Reeve 1\n",
      "@relibertad 'Templanza 1\n",
      "Detalhe 1\n",
      "Barçaleza 1\n",
      "Mis 1\n",
      "Nossa 1\n",
      "@kayro63 1\n",
      "👍👍👍👍 1\n",
      "Clebão 1\n",
      "Vinícius ⚽ 1\n",
      "Rogerio Ceni 1\n",
      "Vin 1\n",
      "Conto 1\n",
      "Professor Gut 1\n",
      "Samba 1\n",
      "Mário Thiago 1\n",
      "Abrazo 1\n",
      "Brun 1\n",
      "😔 1\n",
      "Charles do Ceará 1\n",
      "Vinícius Goes 1\n",
      "Fabio Sanches 1\n",
      "David Moyes 1\n",
      "Vinicius do Ceará 🤪 1\n",
      "🅰 1\n",
      "@fogareu00 1\n",
      "Jajá o 1\n",
      "Luiz Flávio 1\n",
      "NENHUMA 1\n",
      "Mucha 1\n",
      "Viniccius 1\n",
      "Acertei 1\n",
      "Les 1\n",
      "🙅🏾‍♀️🙅🏾‍♀️🙅🏾‍♀️ 1\n",
      "Perder 1\n",
      "Dün 1\n",
      "Kkk 1\n",
      "Davi 1\n",
      "🤡🤡🤡🤡🤡🤡🤡🤡 1\n",
      "🤡🤡🤡🤡 1\n",
      "Karamasov 1\n",
      "Dostoyevski 1\n",
      "QEPD😞 1\n",
      "Ceará S 1\n",
      "Ednardo 1\n",
      "Sérgio Moro 1\n",
      "Gabriel do Fortaleza 1\n",
      "Anem Fortaleza 1\n",
      "João o Fortaleza 1\n",
      "@lucaascartaxo Pior 1\n",
      "Pra 1\n",
      "Che Che Súper Wanda 1\n",
      "Mateus Gonçalves do Ceará 1\n",
      "Chupa 1\n",
      "Valhalla 1\n",
      "Vinícius Vina 1\n",
      "Louis Tomlinson 1\n",
      "INCOMPLETOS 1\n",
      "Maduro 1\n",
      "Iti 1\n",
      "Q VC 1\n",
      "Dra Delsa 1\n",
      "@lucaascartaxo Acabei 1\n",
      "Solange Musse y su papá? 1\n",
      "Derrote 1\n",
      "BARBARIZOU 1\n",
      "@_neigrah Botei 1\n",
      "Jorge Moyano Vega 1\n",
      "Juan 1\n",
      "Rojas 1\n",
      "Torcedora 1\n",
      "Bruyne 1\n",
      "Renan Oliveira 1\n",
      "Shaylon 1\n",
      "Guilherme 1\n",
      "Nao 1\n",
      "😌 1\n",
      "Achei 1\n",
      "👊🏽 1\n",
      "Vinicius ⚽️ 44 1\n",
      "Cópia 1\n",
      "https://t.co/NlJBYaUSs7 1\n",
      "Gobernador Roselló de la Fortaleza 1\n",
      "📸 Kely Pereira 1\n",
      "Luiz Otavio 1\n",
      "Cit 1\n",
      "Parec 1\n",
      "🤡🤡🤡🤡🤡🤡🤡 1\n",
      "Breno 1\n",
      "Vinícius 'Vina 1\n",
      "Ritinha 1\n",
      "Cléber do Ceará 🤡🤡🤡🤡 1\n",
      "«Recuerde 1\n",
      "Deitado 1\n",
      "Reyes Católicos 1\n",
      "Isabel 1\n",
      "Samuel 1\n",
      "Solange 1\n",
      "Blas Correas 1\n",
      "Lances 1\n",
      "@lukecastelao 1\n",
      "Mamãe Falei 1\n",
      "@pawsksj 1\n",
      "Pottker 1\n",
      "Bruno Nazário 1\n",
      "Jesús 1\n",
      "Justo 1\n",
      "Ricardinho do Ceará 1\n",
      "Prezada Renata 1\n",
      "Norm 1\n",
      "Schiaretti 1\n",
      "Dale 1\n",
      "Timot 1\n",
      "Léo Pereira 1\n",
      "Ceará Quitero 1\n",
      "Jackson 1\n",
      "Marisa 1\n",
      "Las 1\n",
      "Bella 1\n",
      "Loko 1\n",
      "Gareth Bale 1\n",
      "Sr Noroña 1\n",
      "Sufrimos 1\n",
      "Indultos 1\n",
      "Vai 1\n",
      "Vagabundo 1\n",
      "Confiaré y 1\n",
      "Querías 1\n",
      "@ocomenteiro Poderia 1\n",
      "Japonés Kawasaki 1\n",
      "Câmera 1\n",
      "Roberto 1\n",
      "Papa Inocencio XI 1\n",
      "Cássio 1\n",
      "EstoNoSeraTendencia 1\n",
      "Bate 1\n",
      "Ido dónde quiera 1\n",
      "KKKKKKKKKKKKKKKKKK 1\n",
      "Gina 1\n",
      "Poe 1\n",
      "Hermano 1\n",
      "Bicho 1\n",
      "Ferna 1\n",
      "Nathan 1\n",
      "Aaaaaaaaah 1\n",
      "Jano Cleyson 1\n",
      "Cachaceiro vc 1\n",
      "Daniel Zellhube 1\n",
      "Salmito Filho 1\n",
      "😔😔✋🏻 1\n",
      "Coro Gomes 1\n",
      "Rael Barbosa de Jacarequara Cedral 1\n",
      "Martínez 1\n",
      "Mailton 1\n",
      "Polariza 1\n",
      "También 1\n",
      "Vázquez 1\n",
      "Pohessa 1\n",
      "Melissa 1\n",
      "Véi 1\n",
      "Jesús y 1\n",
      "Rafael Sóbis 1\n",
      "Veronica Bailone Viceintendenta 1\n",
      "Laird Hamilton 1\n",
      "@crvgeloi 1\n",
      "Ceará Fabinho 1\n",
      "W. Oliveira 1\n",
      "Sobis 1\n",
      "Melissa Dios 1\n",
      "Mariano 1\n",
      "Fragapane 1\n",
      "Sóbis 1\n",
      "Sebastián de Belalcázar 1\n",
      "Ricardo Rivera Ardila 1\n",
      "André Henning 1\n",
      "Kkkk 1\n",
      "Cléber 🤡🤡🤡🤡🤡🤡🤡 1\n",
      "Triste 😢 QEPD el 1\n",
      "Juan Carlos Villalba Barragán 1\n",
      "Gabriela Villalba y mucha fortaleza 🕊 1\n",
      "Mujeeer 1\n",
      "Derrubo 1\n",
      "Mariano Vásquez 1\n",
      "Fernando 1\n",
      "@mundodeumleao Tou 1\n",
      "Phatético 1\n",
      "Boletim 1\n",
      "Luan Peres 1\n",
      "Patric Brey 1\n",
      "Giovani 1\n",
      "Ricardo 1\n",
      "Dios es 1\n",
      "F. Baxola 1\n",
      "Darío Cvitanich 1\n",
      "Querido Facundo 1\n",
      "Leí 1\n",
      "Mlr 1\n",
      "Mary 1\n",
      "Matheus 1\n",
      "Mis Oraciones 1\n",
      "Cristo Jesús 'Por lo demás 1\n",
      "Sra. Chávez 1\n",
      "Uds es 1\n",
      "Cléber do Ceará 1\n",
      "Experimenté 1\n",
      "Lupin 1\n",
      "Jigen 1\n",
      "Cerri 1\n",
      "Vc 1\n",
      "Anderson 1\n",
      "Dale Sr. 1\n",
      "Paz y pido 1\n",
      "Alonso 1\n",
      "Piada 1\n",
      "Pleno 1\n",
      "Falhei 1\n",
      "KKKKK 1\n",
      "David Corrêa 1\n",
      "Jesus 1\n",
      "Bia 1\n",
      "Falou 1\n",
      "Retroescavadeira 1\n",
      "Venham 1\n",
      "Fortaleza VAI TOMAR NO CU #CEAxFOR 1\n",
      "Juan Fdo Ocampo 1\n",
      "@eunaosoueuta 1\n"
     ]
    }
   ],
   "source": [
    "sorted_d = sorted(d.items(),key=lambda x:x[1],reverse=True)\n",
    "for i in sorted_d:\n",
    "    print(i[0],i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
